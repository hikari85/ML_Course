{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "749f5c2c-6708-4d3b-b393-08036a15353f",
   "metadata": {},
   "source": [
    "[The disappearing computer](https://www.ted.com/talks/imran_chaudhri_the_disappearing_computer_and_a_world_where_you_can_take_ai_everywhere/c?user_email_address=02b4db6ec1c28f003d0443330ce209ef&lctg=62d19e111c794c328c90ca0e)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4028659c8e756d9c",
   "metadata": {},
   "source": [
    "#### Good AI is complex:  \n",
    "It takes high-quality, clean data;  \n",
    "fine-tuning of foundation models;  \n",
    "thoughtful and responsible roll-out.  \n",
    "“Many companies aren’t in a position to use AI in this way yet.”   hai. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8c1a98-63d3-465b-8a63-956d0aae425e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import os\n",
    "from random import randint\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557c83c79d184f7e",
   "metadata": {},
   "source": [
    "Range, domain, and domain constraint  \n",
    "Variance and standard deviation   \n",
    "Covariance and correlation  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d00bfb-9d2c-4c36-b1d4-f95a1a42c166",
   "metadata": {},
   "source": [
    "---\n",
    "#### Dimensionality   \n",
    "\n",
    "#### Data Normalisation   \n",
    "Normalization scales the data to a standard range.  \n",
    "This prevents a specific feature from having a strong influence on the model’s output.  \n",
    "It ensures that the model is more robust to variations in the data.   \n",
    "\n",
    "Normalization gives equal weights/importance to each variable   \n",
    "so that no single variable steers model performance in one direction  \n",
    "just because they are bigger numbers.  \n",
    "E.g.,, clustering algorithms use distance measures to determine if an observation should belong to a certain cluster.\n",
    "\n",
    "##### MinMaxScaler   \n",
    "y = (x – min) / (max – min)  \n",
    "\n",
    "##### StandardScaler   \n",
    "y = (x – mean) / standard_deviation  \n",
    "\n",
    "Also, called **z score** $z = \\frac{x - \\mu}{\\sigma}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af555cad77481c",
   "metadata": {},
   "source": [
    "##### Task   \n",
    "Using Excel, find   \n",
    "- normalised values of height and weight of your class.      \n",
    "- standard normalised values of height and weight of your class.         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd711f9-face-435c-9668-7bc2c854e4fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel('../Data/some_girls.xlsx')\n",
    "df1 = df[['Name','Height_cm', 'Weight_Kg', 'BMI', 'Age']]\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbae989-0519-4fd8-9ac5-3e15023f3b96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# y = (x – min) / (max – min)\n",
    "df1['ht_minMax'] = (df1.Height_cm - df1.Height_cm.min()) / (df1.Height_cm.max() - df1.Height_cm.min())\n",
    "df1['wt_minMax'] = (df1.Weight_Kg - df1.Weight_Kg.min()) / (df1.Weight_Kg.max() - df1.Weight_Kg.min())\n",
    "df1['bmi_minMax'] = (df1.BMI - df1.BMI.min()) / (df1.BMI.max() - df1.BMI.min())\n",
    "# df1.sort_values('Weight_Kg')\n",
    "# df1.sort_values('Height_cm')\n",
    "df1.sort_values('BMI')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8371d55d-3725-41b1-8708-e1057e6fa4d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# y = (x – mean) / standard_deviation\n",
    "df2 = df[['Name','Height_cm', 'Weight_Kg', 'BMI', 'Age']]\n",
    "df2['ht_scaled'] = (df2.Height_cm - df2.Height_cm.mean()) / df2.Height_cm.std()\n",
    "df2['wt_scaled'] = (df2.Weight_Kg - df2.Weight_Kg.mean()) / df2.Weight_Kg.std()\n",
    "df2['bmi_scaled'] = (df2.BMI - df2.BMI.mean()) / df2.BMI.std()\n",
    "df2.sort_values('Weight_Kg')\n",
    "# df2.sort_values('Height_cm')\n",
    "# df2.sort_values('BMI')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86770f74-170e-4cc4-8c5a-6e23a8901dac",
   "metadata": {},
   "source": [
    "##### Normalise using Scikit-Learn function   \n",
    "`scaler.fit_transform(column)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eca5b9-5e6a-44f9-89f4-723dd92a5a1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "df3 = df[['Name','Height_cm', 'Weight_Kg', 'BMI', 'Age']]\n",
    "\n",
    "df3['Weight_Kg_skl'] = scaler.fit_transform(df3[['Weight_Kg']])\n",
    "df3['Height_cm_skl'] = scaler.fit_transform(df3[['Height_cm']])\n",
    "df3['BMI_skl'] = scaler.fit_transform(df3[['BMI']])\n",
    "df3.sort_values('Weight_Kg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef0ea8e-acd1-475f-951b-1ae940d38d2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "  \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "df4 = df[['Name','Height_cm', 'Weight_Kg', 'BMI', 'Age']]\n",
    "\n",
    "df4['Weight_Kg_skl'] = scaler.fit_transform(df4[['Weight_Kg']])\n",
    "df4['Height_cm_skl'] = scaler.fit_transform(df4[['Height_cm']])\n",
    "df4['BMI_skl'] = scaler.fit_transform(df4[['BMI']])\n",
    "df4.sort_values('Weight_Kg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07da6cdc-8871-447e-bb62-2b49f4b3a35f",
   "metadata": {},
   "source": [
    "---  \n",
    "##### Example Data Reduction  \n",
    "We can take any Normal Distribution and convert it to The Standard Normal Distribution.  \n",
    "z-score in Standard Normal Distribution gives the measure of distance from the mean in standard deviations.\n",
    "\n",
    "$$z = \\frac{x - \\mu}{\\sigma}$$  \n",
    "```\n",
    "z is the \"z-score\" (Standard Score)\n",
    "x is the value to be standardized\n",
    "μ ('mu\") is the mean\n",
    "σ (\"sigma\") is the standard deviation\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4750403-dcac-40af-b5ca-67dfa3cf1973",
   "metadata": {},
   "source": [
    "##### Task   \n",
    "Using your data, in a spreadsheet create z scores for numerical values.   \n",
    "Compare them with the z-scores using **`stats.zscore()`**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76570e0d-6d28-4f1a-8c6b-65064e805ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bc3ddf-1420-45ba-b756-b34b2ac82b7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel('../Data/Six_Schools.xlsx', sheet_name='KSP21')\n",
    "df.describe()                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4fa7f8-b7fb-40ce-9c64-d96f06ee2283",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the Z-scores\n",
    "df['age_Z'] = stats.zscore(df.Age)\n",
    "df[['Age','age_Z']].sort_values('Age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85d77ed-5eab-449f-a85c-e0fb14fe9cdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a threshold for outlier detection\n",
    "threshold = 3\n",
    "df[df.age_Z < threshold].sort_values('Age')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029f9ffc-2d55-438c-8579-12637551b72c",
   "metadata": {},
   "source": [
    "#### When to standardise   \n",
    "If the distribution of the quantity is **normal**, then **standardise**,   \n",
    "**else normalise**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c984369b-207c-4f79-aeff-2d53125df20b",
   "metadata": {},
   "source": [
    "[Sonar Project](https://raw.githubusercontent.com/jbrownlee/Datasets/master/sonar.names)  \n",
    "[Sonar Data](https://raw.githubusercontent.com/jbrownlee/Datasets/master/sonar.csv)   \n",
    "\n",
    "shape = (208, 60)  \n",
    "Curated and much used sample data.  \n",
    "\n",
    "KNN Accuracy:  \n",
    "```\n",
    "with raw data: 0.797   \n",
    "min-max scale: 0.813   \n",
    "standardised : 0.810   \n",
    "```  \n",
    "\n",
    "Scaling may or may not result in significant improvement in accuracy.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865d844e-4fb5-4c1a-8ec1-34a15a1e1413",
   "metadata": {},
   "source": [
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5707d4-2398-4d3f-88bd-822d1c779a53",
   "metadata": {},
   "source": [
    "#### Experiment  \n",
    "1. Create a dataset   \n",
    "2. Cluster the data as is using K-mean   \n",
    "3. Doing the same thing on the standardized data yields a totally different result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856e7698-f33a-493e-b9d4-87f9493495cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def random_2D_data(x,y,size):\n",
    "    x = (np.random.randn(size)/3.5)+x\n",
    "    y = (np.random.randn(size)*3.5)+y\n",
    "    return x,y\n",
    "x1,y1 = random_2D_data(2,20,50)\n",
    "x2,y2 = random_2D_data(2,-20,50)\n",
    "x3,y3 = random_2D_data(-2,20,50)\n",
    "x4,y4 = random_2D_data(-2,-20,50)\n",
    "x = np.concatenate((x1,x2,x3,x4))\n",
    "y = np.concatenate((y1,y2,y3,y4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f784669e-4f6f-453e-b96b-1a38ad3dbe32",
   "metadata": {},
   "source": [
    "#### Scaled values    \n",
    "In case of correlation of height/weight with BMI, difference is insignificant with and without scaling.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc4d7db-5c05-4fe6-80bf-20a0ac939e3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = [7.00, 3.50]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "f, axes = plt.subplots(1, 2)\n",
    "df = pd.read_excel('../Data/some_girls.xlsx')\n",
    "sns.regplot(x='Height_cm', y='BMI',data=df4, ax=axes[0])\n",
    "sns.regplot(x='Height_cm_skl', y='BMI_skl',data=df4, ax=axes[1]);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ba5814-8f65-4ce4-af13-034a50974797",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df4[''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313bd14c-713b-4227-94f7-3a8d90cc2c76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(1, 2)\n",
    "df = pd.read_excel('../Data/some_girls.xlsx')\n",
    "sns.regplot(x='Weight_Kg', y='BMI',data=df4, ax=axes[0])\n",
    "sns.regplot(x='Weight_Kg_skl', y='BMI_skl',data=df4, ax=axes[1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b973f8-905e-455a-8c24-e280f81dcf62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df4.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d649f296-9d58-4fa8-aeb3-22a3a050621b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(df4.Height_cm.corr(df4.BMI))\n",
    "print(df4.Height_cm_skl.corr(df4.BMI_skl))\n",
    "print()\n",
    "print(df4.Weight_Kg.corr(df4.BMI))\n",
    "print(df4.Weight_Kg_skl.corr(df4.BMI_skl))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b932a5-5d05-4eb1-b8c0-9b5cdee7911e",
   "metadata": {},
   "source": [
    "#### Feature Selection\n",
    "Filter irrelevant or redundant features from your dataset.  \n",
    "Feature selection keeps a subset of the original features while feature extraction creates new ones.  \n",
    "Some supervised algorithms have built-in feature selection, such as Regularized Regression and Random Forests.  \n",
    "Feature selection can be unsupervised (e.g. Variance Thresholds) or supervised (e.g. Genetic Algorithms).  \n",
    "You can also combine multiple methods if needed.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0712f47f-09a4-4251-aca4-f4fe246130c3",
   "metadata": {},
   "source": [
    "##### Typically Needed  \n",
    "1. Linear Regression   \n",
    "2. Logistic Regression  \n",
    "3. Support Vector Machines\n",
    "4. Neural Networks  \n",
    "5. k-Nearest Neighbors  \n",
    "6. KMeans Clustering  \n",
    "7. Principal Component Analysis\n",
    "\n",
    "##### Typically Not Needed   \n",
    "1. Decision Trees  \n",
    "2. Random Forests  \n",
    "3. Naive Bayes  \n",
    "4. Gradient Boosting  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69739e2-6edf-4864-a167-89414b8affa3",
   "metadata": {},
   "source": [
    "\n",
    "#### Variance Thresholds  \n",
    "Variance thresholds remove features whose values don't change much from observation to observation (i.e. their variance falls below a threshold). These features provide little value.  \n",
    "E.g., In the dataset for the class, the 'Age' and 'Gender' features can be eliminated without loss in information.  \n",
    "\n",
    "Because variance is dependent on scale, you should always normalize your features first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057113b0-165f-49ee-af76-56517dafdb83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "df_temp = df4[['Name']]\n",
    "df5 = df4[['Age','Weight_Kg_skl','Height_cm_skl','BMI_skl']]\n",
    "print(f\"\\nVariances of columns: \\n\\n{df5.agg(['var'])}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a34af84-7e90-425a-8c41-2937a7c050d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sel = VarianceThreshold(threshold = 0.5)\n",
    "df5 = sel.fit_transform(df5.iloc[:,1:])\n",
    "# pd.DataFrame(list(df5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bd75fb-a8c6-4644-84ff-e354fca7fb23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(df5, columns=['Weight', 'Height', 'BMI_skl'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1becc204-646c-4483-84b4-ee9e714a62dd",
   "metadata": {},
   "source": [
    "#### Task   \n",
    "Check variances for age in different datasets in Seven Schools   \n",
    "In each, which columns may be excluded from the dataset while modeling ML?   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67391c4-22a0-4764-9dbc-f2f97628a5fb",
   "metadata": {},
   "source": [
    "### Model Selection  \n",
    "- For any prediction problem, there are many algorithms and methods available - decision trees, random forests, neural networks, and more   \n",
    "- Model evaluation and selection is done by evaluating model performance on a validation dataset  \n",
    "- Holdout validation: Partition available data into a training dataset and a holdout; evaluate model performance on holdout  \n",
    "- Cross-validation: Create a number of partitions (validation datasets) from the\n",
    "training dataset; fit model to the training dataset (sans the validation data);\n",
    "evaluate model against each validation dataset; repeat with each validation set\n",
    "and average results to obtain the cross-validation error  \n",
    "\n",
    "##### Data vs. Model   \n",
    "- Often Data > Methods\n",
    "     - Microsoft researchers (Banco and Brill) evaluated performance of multiple models for a language understanding task  \n",
    "     - Varied size of training dataset (up to 1B words)\n",
    "     - Among modern methods, performance differences between algorithms are relatively small when compared to differences between same algorithms with more/less data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e5cbf3-acec-4f8a-bba3-a8715ab5d377",
   "metadata": {},
   "source": [
    "##### Batch   \n",
    "If the training dataset is large, several batches are made.  \n",
    "E.g., a dataset with 2000 examples is divided into 4 batches of 500 each.   \n",
    "Batch Size = 500/  \n",
    "This would result in 4 **iterations** in one training epoch.  \n",
    "##### Epoch   \n",
    "The entire dataset is passed once.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1394584e-d159-448c-bb10-36692c7f50a3",
   "metadata": {},
   "source": [
    "----\n",
    "#### Equations in Data Science   \n",
    "1. Gradient Descent: An optimization algorithm used to minimize the cost function. It helps us find the optimal parameters for ML models.  \n",
    "2. Normal Distribution: A probability distribution that forms a bell curve and is often used to model and analyze data in statistics.\n",
    "3. Sigmoid: A function that maps input values to a range between 0 and 1. It is commonly used in logistic regression to make predictions.\n",
    "4. Linear Regression: A statistical model used to model a linear relationship between independent and dependent variables.\n",
    "5. Cosine Similarity: A measure that calculates the cosine of the angle between two vectors. It is typically used to determine the similarity between data points.\n",
    "6. Naive Bayes: A probabilistic classifier based on the Bayes theorem. It assumes independence between features and is often used in classification tasks.\n",
    "7. KMeans: The most popular clustering algorithm that is used to partition data points into distinct groups.\n",
    "8. Log Loss: A loss function used to evaluate the performance of classification models using output probabilities.\n",
    "9. MSE (Mean Squared Error): A metric that measures the average squared difference between predicted and actual values. It is commonly used to assess regression models.\n",
    "10. MSE + L2 Regularization: An extension of MSE that includes L2 regularization. It is used to prevent overfitting.\n",
    "11. Entropy: A measure of the uncertainty or randomness of a random variable. It is often utilized in decision trees.\n",
    "12. Softmax: A function that normalizes a set of values into probabilities. It is commonly used in multiclass classification problems.\n",
    "13. Ordinary Least Squares: A method for estimating the parameters in linear regression models by minimizing the sum of squared residuals.\n",
    "14. Correlation: A statistical measure that quantifies the strength and direction of the linear relationship between two variables.\n",
    "14. Z-score: A standardized value that indicates how many standard deviations a data point is from the mean.\n",
    "15. MLE (Maximum Likelihood Estimation): A method for estimating the parameters of a statistical model by maximizing the likelihood of the observed data.\n",
    "16. Eigen Vectors: The non-zero vectors that do not change their direction when a linear transformation is applied. It is widely used in dimensionality reduction techniques.\n",
    "17. R2 (R-squared): A statistical measure that represents the proportion of variance explained by a regression model, indicating its predictive power.\n",
    "18. F1 Score: A metric that combines precision and recall to evaluate the performance of binary classification models.\n",
    "19. Expected Value: The weighted average value of a random variable, calculated by multiplying each possible outcome by its probability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb54115-a940-4de6-bc8e-9de2af25f157",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('../Figures/equations_DS.png', width=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6746fbb5-4001-4eab-8765-9cc059db8688",
   "metadata": {},
   "source": [
    "   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
