{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8475f4d45a74565e",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849a0742358814c9",
   "metadata": {},
   "source": [
    "CO2: Improve model performance of multiple linear regression using grid search hyperparameter tuning technique\t\t\t\t\t\t\t\t\t\t\t\t3 Hours\t  \n",
    "1. Hyperparameters, \n",
    "2. common hyperparameters with examples, and \n",
    "3. hyperparameter tuning techniques:  \n",
    "    - grid search, \n",
    "    - random search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b64fef-79b2-4e36-a12c-905a9e387708",
   "metadata": {},
   "source": [
    "#### Grid Search   \n",
    "**Use Keras Models in scikit-learn**: Keras models can be used in scikit-learn by wrapping them with the KerasClassifier or KerasRegressor class from the module SciKeras.   \n",
    "\n",
    "Grid search is a model hyperparameter optimization technique.\n",
    "In scikit-learn, this technique is provided in the GridSearchCV class.\n",
    "\n",
    "When constructing this class, provide a dictionary of hyperparameters to evaluate in the param_grid argument.  \n",
    "This is a map of the model parameter name and an array of values to try.   \n",
    "\n",
    "By default, accuracy score is optimised,  \n",
    "but we can specify other scores in the score argument of the GridSearchCV constructor.  \n",
    "\n",
    "The GridSearchCV process constructs and evaluates one model for each combination of parameters.  \n",
    "It uses cross validation to evaluate each individual model."
   ]
  },
  {
   "cell_type": "raw",
   "id": "59bf8407-3c77-4944-bcd6-0c8b59c249ca",
   "metadata": {},
   "source": [
    "scikeras is Scikit-Learn compatible wrappers for Keras Models.  \n",
    "\n",
    "! pip install scikeras[tensorflow]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b03941-0640-42bf-8369-572384e96071",
   "metadata": {},
   "source": [
    "Pima Indians onset of diabetes classification dataset is used in the example below.  \n",
    "This is an easy to work with small dataset with all numerical attributes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e09bdb-586b-4a89-87c5-4bd7ed1da84e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use scikit-learn to grid search the batch size and epochs\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44ed4e5-74c3-4650-9337-e1a2673cbdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    " model = Sequential()\n",
    " model.add(Dense(12, input_shape=(8,), activation='relu'))\n",
    " model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    " model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    " return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a997ef-4685-46b9-a0da-537d2d88b84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "dataset = np.loadtxt(\"../Data/pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "\n",
    "model = KerasClassifier(model=create_model, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "batch_size = [10, 20, 40, 60, 80, 100]\n",
    "epochs = [10, 50, 100]\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X, Y)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5fc849-8c7f-49f0-93e4-45258b3a3450",
   "metadata": {},
   "source": [
    "We can tune   \n",
    "1. Batch Size and Number of Epochs   \n",
    "2. Training Optimization Algorithm - SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam   \n",
    "3. Learning Rate and Momentum   \n",
    "4. Network Weight Initialization   \n",
    "5. Neuron Activation Function - softmax, softplus, softsign, relu, tanh, sigmoid, hard_sigmoid, linear   \n",
    "6. Dropout Regularization   \n",
    "7. Number of Neurons in the Hidden Layer   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8c1d0a-dd2b-4ed8-9f7d-9a69f8dd5e25",
   "metadata": {},
   "source": [
    "#### Random Search   \n",
    "Define a search space as a bounded domain of hyperparameter values and randomly sample points in that domain.   \n",
    "\n",
    "\n",
    "##### random search logistic regression model on the sonar dataset   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b0812f-d29e-4eca-a798-0db37ee770e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.stats import loguniform\n",
    "from pandas import read_csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d47bdd0-d991-436d-9531-154649be21a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/sonar.csv'\n",
    "data = read_csv(url, header=None).values\n",
    "X, y = data[:, :-1], data[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bd4353-6ef3-42ec-b6b9-56e8fe8d0c60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "\n",
    "# define evaluation\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# define search space\n",
    "space = dict()\n",
    "space['solver'] = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "space['penalty'] = ['none', 'l1', 'l2', 'elasticnet']\n",
    "space['C'] = loguniform(1e-5, 100)\n",
    "\n",
    "# define search\n",
    "search = RandomizedSearchCV(model, space, n_iter=500, scoring='accuracy', n_jobs=-1, cv=cv, random_state=1)\n",
    "\n",
    "# execute search\n",
    "result = search.fit(X, y)\n",
    "\n",
    "# summarize result\n",
    "print('Best Score: %s' % result.best_score_)\n",
    "print('Best Hyperparameters: %s' % result.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
