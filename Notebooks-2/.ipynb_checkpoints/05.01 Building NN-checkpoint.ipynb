{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb65337472d80782",
   "metadata": {},
   "source": [
    "#### Deep Nural Network    \n",
    "Depth in terms of the number of layers.      \n",
    "\n",
    "![](../Figures/cnn-4.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a96629e-e1e1-41e4-8330-574eb4073ba8",
   "metadata": {},
   "source": [
    "#### Activation Functions in DNN  \n",
    "##### Sigmoid function   \n",
    "Defined as $\\frac{1}{1 + e^{-x}}$  where x is the input value and e is the mathematical constant of 2.718.  \n",
    "The function maps any input ($-\\infty$ to $+\\infty$) to a value between 0 and 1, making it useful for binary classification and logistic regression problems.   \n",
    "A neuron that uses a sigmoid function as an activation function is called a sigmoid unit.  \n",
    "\n",
    "##### Hyperbolic tangent activation function   \n",
    "$$\\frac{e^x - e^{-x}}{e^x + e^{-x}}$$  \n",
    "\n",
    "##### ReLU (Rectified Linear Unit)  \n",
    "\n",
    "\n",
    "##### Softmax  \n",
    "Outputs a vector of values that sum up to 1.  \n",
    "Each value indicates the probability of class membership.  \n",
    "\n",
    "|Network Type|Activation Function|  \n",
    "|-|-|    \n",
    "|Multilayer Perceptron|ReLU|  \n",
    "|Convolutional Neural Net|ReLU|  \n",
    "|Recurrent Neural Net|Sigmoid or Tanh|   \n",
    "\n",
    "#### Output Activation Function   \n",
    "\n",
    "|Problem Type|Activation Function|  \n",
    "|-|-|    \n",
    "|Binary Classification|Sigmoid|  \n",
    "|Multiclass Classification Net|Softmax|  \n",
    "|Mutlilabel Classification|Sigmoid|  \n",
    "|Linear Regression|Linear|   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64263c52-9e56-4f75-9878-341941f07d53",
   "metadata": {},
   "source": [
    "![](../Figures/DNN_Activation_Fn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a319d50e94820d8",
   "metadata": {},
   "source": [
    "#### Designing Neural Network   \n",
    "![](../Figures/buildNN-1.png)   \n",
    "\n",
    "Serialise perceptron.  \n",
    "Input, hidden, and output layers.  \n",
    "Each has its own weights and its own sigmoid operation (activation).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d72d21d51dabd",
   "metadata": {},
   "source": [
    "A layer with 10 nodes (neurons) has a 10 column matrix. Any number of rows.   \n",
    "The diagram represents one row. Stacked are nodes equal to the number of rows.   \n",
    "The functions in between the layers are called **activation function** of the network.     \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1945f734fb92c39c",
   "metadata": {},
   "source": [
    "**Input Nodes** one per feature (column) of dataset. 784 pixels + bias = 785 nodes. NMIST Input matrix (60000, 785)  \n",
    "**Output Nodes** one per label (class) of dataset. Output matrix (60000, 10).   \n",
    "**Hidden Nodes** Part of design and tuning. Say, 201.    \n",
    "\n",
    "##### Weights   \n",
    "The three perceptron network above has two matrices of weights.  \n",
    "Each matrix of weights in a neural network has as many rows as its input elements and    \n",
    "as many columns as it output elements.  \n",
    "In the above diagram, w1 is (n, d) and w2 is (d, k).  \n",
    "Operations in the network: \n",
    "       $$H = sigmoid(X . W1)$$\n",
    "       $$\\hat{Y} = sigmoid(H . W2)$$\n",
    "\n",
    "The single perceptron has one row per input variable and one column per class.     \n",
    "**A network for NMIST**   \n",
    "\n",
    "![](../Figures/cnn-3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26ad46de12c4eb1",
   "metadata": {},
   "source": [
    "#### Softmax   \n",
    "Activation function before the output layer is softmax most of the times.  \n",
    "\n",
    "Softmax takes an array of numbers, called _logits_\n",
    "$softmax(l_i) = \\frac{e^{l_i}}{\\sum e^l}$   \n",
    "take the exponential of each logit and divide it by the summed exponentials of all the logits.  \n",
    "Like the sigmoid, softmax returns an array where each element is between 0 and 1.  \n",
    "The sum of its output is always 1.   \n",
    "\n",
    "[1.6, 3.1, 0.5] -> softmax is [0.17198205, 0.77077009, 0.05724785]      \n",
    "Chances of the item belonging to the second class is77%   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5ce6680f2a5589",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-10T03:43:49.489028Z",
     "start_time": "2024-04-10T03:43:49.475614500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e01518a8bb7e2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T01:39:05.593346100Z",
     "start_time": "2024-04-08T01:39:05.560564800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def softmax(logits):\n",
    "    exponentials = np.exp(logits)\n",
    "    return exponentials/np.sum(exponentials, axis=1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855a32fcf6fe94cd",
   "metadata": {},
   "source": [
    "for MNIST logits would be (60000, 10) matrix.   \n",
    "axis=1 means calculate the sum by row and not for the entire matrix.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd27b54e91d96c00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T01:39:31.433973100Z",
     "start_time": "2024-04-08T01:39:31.402109500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample = np.array([[0.3, 0.8, 0.2], [0.1,0.9,0.1]])\n",
    "exponentials = np.exp(sample) \n",
    "print(exponentials); print()\n",
    "print(np.sum(exponentials, axis=1)); print()\n",
    "print(softmax(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de82a7967fd1230",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T01:40:08.986959800Z",
     "start_time": "2024-04-08T01:40:08.945636800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1e5ef858a7fdf",
   "metadata": {},
   "source": [
    "#### Forward Propagation   \n",
    "Code for prediction in a perceptron is   \n",
    "```\n",
    "def predict(X, w):\n",
    "    weighted_sum = np.matmul(X, w)\n",
    "    return sigmoid(weighted_sum)\n",
    "```\n",
    "To propagate the data forward,   \n",
    "1. similar to perceptron's prediction   \n",
    "    h = sigmoid(np.matmul(prepend_bias(X), w1))    \n",
    "    this calculates the hidden layer.  \n",
    "2. repeat the process to calculate the output layer   \n",
    "    y_hat = softmax(np.matmul(prepend_bias(h), w2))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1514f601334d99f2",
   "metadata": {},
   "source": [
    "##### Refactor Code: Extract function      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a7be7f2ceb0b61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T01:40:35.360185600Z",
     "start_time": "2024-04-08T01:40:35.345217700Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepend_bias(X):\n",
    "    return np.insert(X, 0, 1, axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258d38eb3f454021",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T01:41:15.582780400Z",
     "start_time": "2024-04-08T01:41:15.509417600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def forward(X, w1, w2):\n",
    "    h = sigmoid(np.matmul(prepend_bias(X), w1))\n",
    "    y_hat = softmax(np.matmul(prepend_bias(h), w2))  \n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73046bddceb862d",
   "metadata": {},
   "source": [
    "#### classify() function  \n",
    "Neural network's `classify()` is similar to that of perceptron except that   \n",
    "it takes two weights instead of one.   \n",
    "MNIST output layer is (60000, 10) matrix.    \n",
    "argmax() makes it (60000, 1) matrix.  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf00cf35176fda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T01:41:23.773497700Z",
     "start_time": "2024-04-08T01:41:23.755606700Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def classify(X, w1, w2):\n",
    "    y_hat = forward(X, w1, w2)\n",
    "    labels = np.argmax(y_hat, axis=1)\n",
    "    return labels.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7853f7dd98062e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T01:41:28.372094500Z",
     "start_time": "2024-04-08T01:41:28.341716600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def report(iteration, X_train, Y_train, X_test, Y_test, w1, w2):\n",
    "    y_hat = forward(X_train, w1, w2)\n",
    "    training_loss = loss(Y_train, y_hat)\n",
    "    classifications = classify(X_test, w1, w2)\n",
    "    accuracy = np.average(classifications == Y_test)*100\n",
    "    print(f\"{iteration} {training_loss} {accuracy}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da93986e3c8888f",
   "metadata": {},
   "source": [
    "#### Cross Entropy Loss   \n",
    "softmax and cross-entropy loss pair well.  \n",
    "$$L = - \\frac{1}{m}\\sum y_i\\ .\\ log(\\hat{y}_i)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3304d352e14815b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T01:42:11.225598200Z",
     "start_time": "2024-04-08T01:42:11.205409200Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss(Y, y_hat):\n",
    "    return -np.sum(Y * np.log(y_hat)) / Y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350852609e0c39e5",
   "metadata": {},
   "source": [
    "#### Complete Code   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9493177acff02a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T01:42:23.151821200Z",
     "start_time": "2024-04-08T01:42:23.136778900Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def softmax(logits):\n",
    "    exponentials = np.exp(logits)\n",
    "    return exponentials / np.sum(exponentials, axis=1).reshape(-1, 1)\n",
    "\n",
    "def loss(Y, y_hat):\n",
    "    return -np.sum(Y * np.log(y_hat)) / Y.shape[0]\n",
    "\n",
    "def prepend_bias(X):\n",
    "    return np.insert(X, 0, 1, axis=1)\n",
    "\n",
    "def forward(X, w1, w2):\n",
    "    h = sigmoid(np.matmul(prepend_bias(X), w1))\n",
    "    y_hat = softmax(np.matmul(prepend_bias(h), w2))\n",
    "    return y_hat\n",
    "\n",
    "def classify(X, w1, w2):\n",
    "    y_hat = forward(X, w1, w2)\n",
    "    labels = np.argmax(y_hat, axis=1)\n",
    "    return labels.reshape(-1, 1)\n",
    "\n",
    "def report(iteration, X_train, Y_train, X_test, Y_test, w1, w2):\n",
    "    y_hat = forward(X_train, w1, w2)\n",
    "    training_loss = loss(Y_train, y_hat)\n",
    "    classifications = classify(X_test, w1, w2)\n",
    "    accuracy = np.average(classifications == Y_test) * 100.0\n",
    "    print(f\"{iteration:5} {training_loss:.6f} {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279b908fbaab1a5d",
   "metadata": {},
   "source": [
    "#### Backpropagation   \n",
    "Calculate the gradients of a neural network's loss with respect to weights using the chain rule.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744e67b9ba58b766",
   "metadata": {},
   "source": [
    "### Solution with TensorFlow   \n",
    "Using keras wrapper  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a87bbf91b2ec4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-10T03:42:36.641421900Z",
     "start_time": "2024-04-10T03:42:25.154237Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b2053eaacc5a3d",
   "metadata": {},
   "source": [
    "#### Classify MNIST dataset with DNN   \n",
    "The MNIST dataset, a set of four Numpy arrays, comes preloaded in Keras.  \n",
    "Load the datasets and normlise (min-max).   \n",
    "Division by 255 normalises the data as well as converts to floating-point numbers.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd92799fa7164a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-10T03:42:56.853179200Z",
     "start_time": "2024-04-10T03:42:56.193909Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# normalise input data \n",
    "x_train = x_train / 255.0 \n",
    "x_test = x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deae8cc0b00aad4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-10T03:43:21.046004500Z",
     "start_time": "2024-04-10T03:43:20.985065500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"{x_train.shape = }\")\n",
    "print(f\"{x_test.shape  = }\")\n",
    "print(f\"{y_train = }\")\n",
    "print(f\"{y_test  = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb6c097f32049d1",
   "metadata": {},
   "source": [
    "1. Feed the neural network the training data - train_images and train_labels.     \n",
    "2. The network will learn to associate images and labels.  \n",
    "3. Ask the network to produce predictions for test_images.  \n",
    "4. Verify if these predictions match the labels from test_labels.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e280f17b54cc6e",
   "metadata": {},
   "source": [
    "#### Make a model (neural network)      \n",
    "Two densely connected layers.   \n",
    "The last layer is a 10-way softmax layer. It will return probability scores. Each score will be the probability that the curent digit image belongs to one of the 10 digit classes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308433994478b574",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-10T03:43:34.189261500Z",
     "start_time": "2024-04-10T03:43:33.482043400Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a47a57ec8b8e0d",
   "metadata": {},
   "source": [
    "Sequential layers: each layer has one input tensor and one outputtensor.  \n",
    "We have used Flatten, Dense, and Dropout layers.  \n",
    "\n",
    "For each example, the model returns a vector of logits or log-odds scores, one for each class.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1546b054cc632cfe",
   "metadata": {},
   "source": [
    "#### Train the model    \n",
    "1.  Before you start training, configure and compile the model using Keras Model.compile.  \n",
    "Set the optimizer class to adam, set the loss to the loss_fn function you defined earlier, and specify a metric to be evaluated for the model by setting the metrics parameter to accuracy.  \n",
    "2. Train and evaluate the model    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9dc827accf3500",
   "metadata": {},
   "source": [
    "##### Optimiser classes  \n",
    "Adadelta, Adafactor, Adagrad, Adam, AdamW, Adamax, \n",
    "Ftrl, Lion, LossScaleOptimizer, Nadam, Optimizer, RMSprop, and SGD   \n",
    "\n",
    "##### Loss Functions   \n",
    "BinaryCrossentropy, BinaryFocalCrossentropy, CTC, CategoricalCrossentropy,    \n",
    "CategoricalFocalCrossentropy, CategoricalHinge, CosineSimilarity, Dice,  \n",
    "Hinge, Huber, KLDivergence, LogCosh, Loss, MeanAbsoluteError,   \n",
    "MeanAbsolutePercentageError, MeanSquaredError, MeanSquaredLogarithmicError,  \n",
    "Poisson, Reduction, SparseCategoricalCrossentropy, SquaredHinge.  \n",
    "\n",
    "##### Metrics   \n",
    "AUC, Accuracy, BinaryAccuracy, BinaryCrossentropy, BinaryIoU, CategoricalAccuracy,   \n",
    "CategoricalCrossentropy, CategoricalHinge, CosineSimilarity, F1Score, FBetaScore,   \n",
    "FalseNegatives, FalsePositives, Hinge, IoU, KLDivergence, LogCoshError, Mean, MeanAbsoluteError,   \n",
    "MeanAbsolutePercentageError, MeanIoU, MeanMetricWrapper, MeanSquaredError, MeanSquaredLogarithmicError,   \n",
    "Metric, OneHotIoU, OneHotMeanIoU, Poisson, Precision, PrecisionAtRecall, R2Score, Recall, RecallAtPrecision,   \n",
    "RootMeanSquaredError, SensitivityAtSpecificity, SparseCategoricalAccuracy, SparseCategoricalCrossentropy,  \n",
    " SparseTopKCategoricalAccuracy, SpecificityAtSensitivity, SquaredHinge, Sum,   \n",
    " TopKCategoricalAccuracy, TrueNegatives, TruePositives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbdae0d636ad896",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-10T03:44:07.427526Z",
     "start_time": "2024-04-10T03:44:07.262175500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5be91fe3d42e648",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9278bb9e4ca8f4c2",
   "metadata": {},
   "source": [
    "#### Validate on test set  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f174bd18e7a976f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-10T03:57:00.909462900Z",
     "start_time": "2024-04-10T03:56:59.616040Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a15bcf-8078-4d81-95fd-ef18d02ef69e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-10T04:17:04.436759500Z",
     "start_time": "2024-04-10T04:17:04.245547700Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "print(y_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4491131c-90b6-427e-8797-9bdc81b5360e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-10T04:17:04.436759500Z",
     "start_time": "2024-04-10T04:17:04.245547700Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num = randint(1,9)\n",
    "print(f\"\\nindex of random test value: {num}; digit: {y_test[:10][num-1]}\\n\")\n",
    "\n",
    "print(f\"\\nPredicted digit: {np.argmax(model.predict(x_test[num-1:num]))}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
