{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decision Tree  \n",
    "```\n",
    "create_branch\n",
    "  check if every item in the dataset is in the same class\n",
    "    if so, return the dataset\n",
    "    else\n",
    "      find the best feature to split the dataset\n",
    "      split the dataset\n",
    "      create a branch node\n",
    "        for each split\n",
    "          call create_branch and add the result to the branch node\n",
    "        return branch node\n",
    "```                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[Example Dataset](https://www.kaggle.com/yersever/500-person-gender-height-weight-bodymassindex?select=500_Person_Gender_Height_Weight_Index.csv) is taken from Kaggle.  \n",
    "This is a synthetic dataset. The gender info is not realistic.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_excel(\"../Data/500_Height_Weight.xlsx\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.heatmap(df.corr(), annot=True);#, cmap='autumn');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels   \n",
    "Index and Gender appears to be categorical, and hence, candidate labels.  \n",
    "However, Gender has poor correlation with other features.  \n",
    "Weight has strong correlation with Index.  \n",
    "Therefore, the label would be index, probably indicating BMI.  \n",
    "\n",
    "Note: the index is strongly correlated only with weight. Effect of height is insignificant. \n",
    "\n",
    "We want to predict if a person is obese.\n",
    "By some inspection, we see that people with an index of 4 or 5 are obese.\n",
    "\n",
    "Create a variable to reflect if a person is obese or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['obese'] = (df.Index >= 4).astype('int')\n",
    "df.head()\n",
    "print(f\"Total: {len(df)}; Obese: {len(df[df.obese==True])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider that people weighing >= 100 kg are most likely obese.  \n",
    "\n",
    "There will be people   \n",
    "- who weigh >= 100 kg and are not obese.   \n",
    "- who weigh < 100 kg are are obese.  \n",
    "\n",
    "The decision tree will continue to create more branches that generate new conditions to “refine” our predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"People weighing < 100 Kg and are obese:      {df[df.obese & (df.Weight < 100)].shape[0]}\")\n",
    "print(f\"People weighing >= 100 Kg and are not obese: {df[~df.obese & (df.Weight >= 100)].shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"People weighing < 80 Kg and are obese:      {df[df.obese & (df.Weight < 80)].shape[0]}\")\n",
    "print(f\"People weighing >= 80 Kg and are not obese: {df[~df.obese & (df.Weight >= 80)].shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a cut at 100, we have higher false negative and lower false positive cases.  \n",
    "Vice versa with a cut at 80.   \n",
    "\n",
    "We would have to further break on the false negative branch.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Impurity and cost functions of a decision tree\n",
    "As in all algorithms, the cost function is the basis of the algorithm.  \n",
    "The two main cost functions of  decision trees are the **Gini index** and **entropy**.\n",
    "\n",
    "**Impurity**: likelihood of the target variable being classified incorrectly.  \n",
    "Choice of the cost functions is based on measuring impurity.\n",
    "\n",
    "In the example above, impurity will include  \n",
    "- the % of people that weigh >=100 kg that are not obese and  \n",
    "- the % of people that weigh <100 kg and are obese.  \n",
    "\n",
    "Every time we make a split and the classification is not perfect, the split is impure.  \n",
    "\n",
    "Not all cuts are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Calculate impurity using the Gini index\n",
    "This index calculates the probability that a specific characteristic will be classified incorrectly when it is randomly selected.\n",
    "\n",
    "The Gini Index is a summary measure of inequality. The Gini coefficient ranges from 0, indicating perfect equality to 1, perfect inequality. The Gini index is calculated as:\n",
    "$$Gini = 1 - \\sum\\limits_{i=1}^{n}(Pi)^2$$\n",
    "\n",
    "Where Pi is the probability of having that class or value.\n",
    "\n",
    "Program a function, considering the input will be a Pandas series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Given a Pandas Series, calculate the Gini Impurity.\n",
    "def gini_impurity(y):\n",
    "    p = y.value_counts()/y.shape[0]\n",
    "    return 1-np.sum(p**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gini Index for NC  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_nc = pd.read_excel('../Data/NC_Student_Data.xlsx')\n",
    "df_nc['Inst'] = 'NC'                                     # All the rows have the same value \n",
    "print(\"                 Gini   std\")\n",
    "for col in ['Sex', 'Height', 'Weight', 'BMI_Cat']:\n",
    "    print(f\"\\t{col:7}  {gini_impurity(df_nc[col]):.2f}  {df_nc[col].std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for col in ['Inst', 'Sex', 'BMI_Cat']:\n",
    "    print(f\"{col:8}{df_nc[col].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gini Index for Sample Dataset   \n",
    "This is the synthetic dataset  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Gini Impurity for\")\n",
    "for col in ['Male', 'Height', 'Weight', 'Index']:\n",
    "    print(f\"\\t{col:6} {gini_impurity(df[col]):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df1 = df[df.obese == 0]\n",
    "\n",
    "print(\"Gini Impurity for the first branch\")\n",
    "for col in ['Male', 'Height', 'Weight', 'Index']:\n",
    "    print(f\"\\t{col:6}: {gini_impurity(df1[col]):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2 = df[df.obese == 1]\n",
    "\n",
    "print(\"Gini Impurity for the second branch\")\n",
    "for col in ['Male', 'Height', 'Weight', 'Index']:\n",
    "    print(f\"\\t{col:6}: {gini_impurity(df2[col]):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The Gini index for the Gender variable is very close to 0.5.\n",
    "This indicates that the Gender variable is very impure.\n",
    "Both the results will have the same proportion of incorrectly classified data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Calculate impurity with entropy\n",
    "Entropy measures impurity or randomness in data points. Entropy is defined by the following formula:\n",
    "$$E(s) =   \\sum\\limits_{i=1}^{c}- p_ilog_2p_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def entropy(y):\n",
    "    a = y.value_counts()/y.shape[0]\n",
    "    return np.sum(-a*np.log2(a+1e-9))\n",
    "\n",
    "print(\"Entropy for\")\n",
    "for col in ['Male', 'Height', 'Weight', 'Index']:\n",
    "    print(f\"\\t{col:6}: {entropy(df[col]):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### How to choose the cuts for decision tree\n",
    "Cuts are compared by impurity.\n",
    "We are interested in comparing the cuts that generate less impurity.\n",
    "For this, Information Gain is used.\n",
    "This metric indicates the improvement when making different partitions and is usually used with entropy.\n",
    "It could also be used with the Gini index, although in that case it would not be called Informaiton Gain.\n",
    "\n",
    "$$InformationGain_{Classification} = E(d)–∑\\frac{|s|}{|d|}E(s)$$\n",
    "$$InformationGain_{Regresion} = Variance(d)–∑\\frac{|s|}{|d|}Variance(s)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def variance(y):\n",
    "    if(len(y) == 1):\n",
    "        return 0\n",
    "    else:\n",
    "        return y.var()\n",
    "\n",
    "def information_gain(y, mask, func=entropy):\n",
    "    '''\n",
    "    It returns the Information Gain of a variable given a loss function.\n",
    "    y: target variable.\n",
    "    mask: split choice.\n",
    "    func: function to be used to calculate Information Gain in case os classification.\n",
    "    '''\n",
    "\n",
    "    a = sum(mask)\n",
    "    b = mask.shape[0] - a\n",
    "\n",
    "    if(a == 0 or b ==0):\n",
    "        ig = 0\n",
    "\n",
    "    else:\n",
    "        if y.dtypes != 'O':\n",
    "            ig = variance(y) - (a/(a+b)* variance(y[mask])) - (b/(a+b)*variance(y[-mask]))\n",
    "        else:\n",
    "            ig = func(y)-a/(a+b)*func(y[mask])-b/(a+b)*func(y[-mask])\n",
    "\n",
    "    return ig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Weight  Info Gain')\n",
    "for weight in [70, 80, 90, 100, 110, 120]:\n",
    "    print(f\"{weight:4}  {information_gain(df['obese'], df['Weight'] >= weight):10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Knowing this, the steps that we need to follow to code a decision tree from scratch in Python are simple:\n",
    "\n",
    "1. Calculate the Information Gain for all variables.\n",
    "2. Choose the split that generates the highest Information Gain as a split.\n",
    "3. Repeat the process until at least one of the conditions set by hyperparameters of the algorithm is not fulfilled.\n",
    "\n",
    "How do we choose which is the best split in the numerical variables?\n",
    "And if there is more than one categorical variable?\n",
    "\n",
    "### How to calculate the best split for a variable\n",
    "For a numeric variable,\n",
    "1. Find all the possible values of the variable.\n",
    "2. For each option, calculate the Information Gain\n",
    "\n",
    "For categorical variables, calculate the Information Gain for all possible combinations of that variable.\n",
    "This is computationally costly if we have a high number of categories. Decision tree algorithms usually accept categorical variables with less than 20 categories.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# Creates all possible combinations from a Pandas Series.\n",
    "def categorical_options(a):\n",
    "    a = a.unique()\n",
    "    options = []\n",
    "    for L in range(0, len(a)+1):\n",
    "        for subset in itertools.combinations(a, L):\n",
    "            subset = list(subset)\n",
    "            options.append(subset)\n",
    "\n",
    "    return options[1:-1]\n",
    "\n",
    "def max_information_gain_split(x, y, func=entropy):\n",
    "    '''\n",
    "    Given a predictor & target variable, returns the best split, the error and the type of variable based on a selected cost function.\n",
    "    x: predictor variable as Pandas Series.\n",
    "    y: target variable as Pandas Series.\n",
    "    func: function to be used to calculate the best split.\n",
    "    '''\n",
    "\n",
    "    split_value = []\n",
    "    ig = []\n",
    "\n",
    "    numeric_variable = True if x.dtypes != 'O' else False\n",
    "\n",
    "    # Create options according to variable type\n",
    "    if numeric_variable:\n",
    "        options = x.sort_values().unique()[1:]\n",
    "    else:\n",
    "        options = categorical_options(x)\n",
    "\n",
    "    # Calculate ig for all values\n",
    "    for val in options:\n",
    "        mask =   x < val if numeric_variable else x.isin(val)\n",
    "        val_ig = information_gain(y, mask, func)\n",
    "        # Append results\n",
    "        ig.append(val_ig)\n",
    "        split_value.append(val)\n",
    "\n",
    "    # Check if there are more than 1 results if not, return False\n",
    "    if len(ig) == 0:\n",
    "        return(None,None,None, False)\n",
    "\n",
    "    else:\n",
    "        # Get results with highest IG\n",
    "        best_ig = max(ig)\n",
    "        best_ig_index = ig.index(best_ig)\n",
    "        best_split = split_value[best_ig_index]\n",
    "        return(best_ig,best_split,numeric_variable, True)\n",
    "\n",
    "weight_ig, weight_split, _, _ = max_information_gain_split(df['Weight'], df['obese'],)\n",
    "\n",
    "print(f\"The best split for Weight is when the variable is less than {weight_split}\")\n",
    "print(f\"Information Gain for that split is: { weight_ig}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The best split generates the highest Information Gain.\n",
    "Calculate the Information Gain for each of the predictor variables of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.drop('obese', axis= 1).apply(max_information_gain_split, y = df['obese'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable with the highest Information Gain is Weight.\n",
    "Therefore, it is the variable to use first to split.\n",
    "In addition, we also have the value on which the split must be performed: 103.\n",
    "\n",
    "The first split will generate two dataframes. If we apply this recursively, we will end up creating the entire decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We'll skip the remaining conceptual discussions   \n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X = df[['Male','Height','Weight']]   # Features\n",
    "y = df.Index                         # target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = list(zip(y_test, y_pred))\n",
    "match = [(x == y) for (x, y) in res if x==y]\n",
    "print(f\"Prediction Accuracy: {100 * len(match) / len(res):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_test)\n",
    "df_test = X_test.copy()\n",
    "df_test['y'] = y_test\n",
    "df_test['y_hat'] = y_pred\n",
    "df_test[df_test.y != df_test.y_hat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "tree.plot_tree(clf);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Decision Tree Regressor  \n",
    "Decision trees can also be applied to regression problems, using the DecisionTreeRegressor class.\n",
    "\n",
    "As in the classification setting, the fit method will take as argument arrays X and y.  \n",
    "Regressor expects  floating point values for y.   \n",
    "Classifier requires integer values for y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nmk = pd.read_excel('../Data/NMKRV_MSc_2023.xlsx')\n",
    "df_nmk.columns\n",
    "X = df_nmk[['Height_cm', 'Weight_Kg']]   # Features\n",
    "y = df_nmk.BMI                         # target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/3, random_state=0)\n",
    "\n",
    "regr = tree.DecisionTreeRegressor()\n",
    "regr = clf.fit(X_train, y_train)\n",
    "predicted = regr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check Accuracy  \n",
    "RMSE  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "df_res = X_test.copy()\n",
    "df_res['y'] = y_test\n",
    "df_res['y_hat'] = predicted\n",
    "df_res['SE'] = (df_res.y - df_res.y_hat)**2\n",
    "print(f\"\\nRMSE = {math.sqrt(df_res.SE.sum())}\")\n",
    "df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nmk.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_nmk[['Height_cm', 'Weight_Kg']]   # Features\n",
    "y = df_nmk.BMI_Cat                         # target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/3, random_state=0)\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()  \n",
    "clf = clf.fit(X_train, y_train)\n",
    "predicted = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(f\"\\nScore = {clf.score(X_test, y_test)}\\n\")\n",
    "print(classification_report(y_test, predicted))      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Tips on practical use](https://scikit-learn.org/stable/modules/tree.html)\n",
    "Decision trees tend to overfit on data with a large number of features.  \n",
    "Getting the right ratio of samples to number of features is important,  \n",
    "since a tree with few samples in high dimensional space is very likely to overfit.\n",
    "\n",
    "Consider performing dimensionality reduction beforehand to   \n",
    "give your tree a better chance of finding features that are discriminative.\n",
    "\n",
    "Understanding the decision tree structure will help in gaining more insights about  \n",
    "how the decision tree makes predictions, which is important for understanding the important features in the data.\n",
    "\n",
    "Visualise your tree as you are training by using the export function.  \n",
    "Use max_depth=3 as an initial tree depth to get a feel for how the tree is fitting to your data,  \n",
    "and then increase the depth.\n",
    "\n",
    "The number of samples required to populate the tree doubles for each additional level the tree grows to.  \n",
    "Use max_depth to control the size of the tree to prevent overfitting.\n",
    "\n",
    "Use min_samples_split or min_samples_leaf to ensure that multiple samples inform every decision in the tree, by controlling which splits will be considered. A very small number will usually mean the tree will overfit, whereas a large number will prevent the tree from learning the data. Try min_samples_leaf=5 as an initial value. If the sample size varies greatly, a float number can be used as percentage in these two parameters. While min_samples_split can create arbitrarily small leaves, min_samples_leaf guarantees that each leaf has a minimum size, avoiding low-variance, over-fit leaf nodes in regression problems. For classification with few classes, min_samples_leaf=1 is often the best choice.\n",
    "\n",
    "Note that min_samples_split considers samples directly and independent of sample_weight, if provided (e.g. a node with m weighted samples is still treated as having exactly m samples). Consider min_weight_fraction_leaf or min_impurity_decrease if accounting for sample weights is required at splits.\n",
    "\n",
    "Balance your dataset before training to prevent the tree from being biased toward the classes that are dominant. Class balancing can be done by sampling an equal number of samples from each class, or preferably by normalizing the sum of the sample weights (sample_weight) for each class to the same value. Also note that weight-based pre-pruning criteria, such as min_weight_fraction_leaf, will then be less biased toward dominant classes than criteria that are not aware of the sample weights, like min_samples_leaf.\n",
    "\n",
    "If the samples are weighted, it will be easier to optimize the tree structure using weight-based pre-pruning criterion such as min_weight_fraction_leaf, which ensure that leaf nodes contain at least a fraction of the overall sum of the sample weights.\n",
    "\n",
    "All decision trees use np.float32 arrays internally. If training data is not in this format, a copy of the dataset will be made.\n",
    "\n",
    "If the input matrix X is very sparse, it is recommended to convert to sparse csc_matrix before calling fit and sparse csr_matrix before calling predict. Training time can be orders of magnitude faster for a sparse matrix input compared to a dense matrix when features have zero values in most of the samples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
