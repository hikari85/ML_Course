{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf6e6c32a9b231d9",
   "metadata": {},
   "source": [
    "### Ensemble Models   \n",
    "Ensemble learning is a general meta approach to machine learning   \n",
    "that seeks better predictive performance by combining the predictions from multiple models.   \n",
    "The three main classes of ensemble learning methods are bagging, stacking, and boosting.  \n",
    "\n",
    "**Bagging**: fit many decision trees on different samples of the same dataset and average the predictions.   \n",
    "**Stacking**: fit many different models types on the same data and use another model to learn how to best combine the predictions.   \n",
    "**Boosting**: add ensemble members sequentially that correct the predictions made by prior models and outputs a weighted average of the predictions.   \n",
    "\n",
    "Ensemble models group a set of **weak learners** and form a **strong learner** by voting or averaging the findings of weak learners.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80081cf6-650d-45bb-b661-4c28f4dc96d4",
   "metadata": {},
   "source": [
    "#### Bagging   \n",
    "\n",
    "Building classifiers from randomly resampled data.  \n",
    "\n",
    "Bootstrap aggregating or bagging. Abbreviation of Bootstrap AGGregatING.   \n",
    "\n",
    "Data is taken from the original dataset S times to make S new datasets, all of the same size.  \n",
    "Each dataset is built by randomly selecting an example from the original **with replacement**.   \n",
    "The <u>bootstrap method</u> is a resampling technique used to estimate statistics on a population  \n",
    "by sampling a dataset with replacement.  \n",
    "\n",
    "After the S datasets are built, a learning algorithm is applied to each one individually.  \n",
    "To classify a new piece of data, S classifiers to the new piece of data and take a majority vote.  \n",
    "\n",
    "These multi datasets are used to train multiple models in parallel.  \n",
    "In <u>regression</u>, the average of all the predictions from different ensemble models is calculated.    \n",
    "In <u>classification</u>, the majority vote gained from the voting mechanism is considered.    \n",
    "Bagging decreases the variance and tunes the prediction to an expected outcome.  \n",
    "\n",
    "The **Random Forest** model uses Bagging, where decision tree models with higher variance are present.  \n",
    "It makes random feature selection to grow trees. Several random trees make a Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7945b56-477b-4b44-ad21-61c8fe963676",
   "metadata": {},
   "source": [
    "##### Bagging  \n",
    "involves fitting many decision trees on different samples of the same dataset  \n",
    "and averaging the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1243c5ee-6cf4-4914-8f1d-3936ed7eb436",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('../Figures/bagging_ensemble.jpg', width=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1810cd-a2c6-4335-a7ad-5b1c78ccf210",
   "metadata": {},
   "source": [
    "##### Stacking   \n",
    "involves fitting many different models types on the same data   \n",
    "and using another model to learn how to best combine the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0715ed-68bc-49af-99df-9067f3eef653",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Image('../Figures/stacking_ensemble.jpg', width=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3a139c-7038-4f99-8e87-2362326f0d98",
   "metadata": {},
   "source": [
    "##### Boosting  \n",
    "involves adding ensemble members sequentially that correct   \n",
    "the predictions made by prior models and outputs a weighted average of the predictions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2b7759-3807-4c1c-a044-daecccfde4f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Image('../Figures/boostng_ensemble.jpg', width=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9a66e1-7c5b-4b42-9779-27e9df09439e",
   "metadata": {},
   "source": [
    "---\n",
    "#### Boosting  \n",
    "\n",
    "Boosting technique is similar to bagging.  \n",
    "In boosting and bagging, we use the same type of classifier.  \n",
    "But in boosting, the different classifiers are trained sequentially.  \n",
    "\n",
    "Each new classifier is trained based on the performance of those already trained.  \n",
    "Boosting makes new classifiers focusing on data that was misclassified by previous classifiers.  \n",
    "It iteratively adjusts the weight of observation as per the last classification.  \n",
    "\n",
    "In boosting, the output is calculated from a weighted sum of all classifiers.  \n",
    "The weights arenâ€™t equal as in bagging but are based on how successful the classifier was in the previous iteration.  \n",
    "\n",
    "Boosting converts a weak learner to a stronger one.  \n",
    "It decreases the bias error and builds strong predictive models.  \n",
    "\n",
    "The **AdaBoost** uses Boosting techniques, where a 50% less error is required to maintain the model.  \n",
    "Here, Boosting can keep or discard a single learner. Otherwise, the iteration is repeated until achieving a better learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf3d44b-409e-4203-9f34-c62a72b21bc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import sklearn\n",
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07768d56-ebc3-4cd3-ba78-f1dcfac47453",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "bc = load_breast_cancer()\n",
    "X = bc.data\n",
    "y = bc.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1, stratify=y)\n",
    "\n",
    "# Pipeline Estimator\n",
    "pipeline = make_pipeline(StandardScaler(),\n",
    "                        LogisticRegression(random_state=1))\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "print(f'Model test Score:     {pipeline.score(X_test, y_test):.3f}\\nModel training Score: {pipeline.score(X_train, y_train):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f22d29-d336-40b8-ac5f-cffbb9a8d449",
   "metadata": {},
   "source": [
    "#### Bagging and Boosting: Similarities  \n",
    "They are ensemble methods focused on getting N learners from a single learner.  \n",
    "They make random sampling and generate several training data sets.   \n",
    "They arrive upon the end decision by making an average of N learners or taking the voting rank of the predictions.  \n",
    "They reduce variance and provide higher stability with minimizing errors.\n",
    "\n",
    "#### Bagging and Boosting: Differences   \n",
    "\n",
    "|bagging|boosting|  \n",
    "|-|-|  \n",
    "|merges the same type of predictions|merges different types of predictions|  \n",
    "|decreases variance, solves over-fitting|decreases bias, not variance|  \n",
    "|each model receives an equal weight|models are weighed based on their performance|  \n",
    "|models are built independently|new models are affected by previous  model's performance|  \n",
    "|training data are drawn randomly with a replacement|new subset is of misclassified elements|  \n",
    "|use if the classifier has a high variance|if the classifier has a high bias|  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2604c8-cb1b-4eab-9338-7dd0ac5ca027",
   "metadata": {},
   "source": [
    "### Some boosting models  \n",
    "1. Adaptive Boosting (AdaBoost)\n",
    "2. eXtreme Gradient Boosting (XGBoost)  \n",
    "3. Light Gradient Boosting Method (LightGBM)\n",
    "4. Category Boosting (CatBoost).   \n",
    "\n",
    "<u>Refer</u>   \n",
    "[pipeline](https://scikit-learn.org/stable/modules/compose.html)   \n",
    "[GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ce4569-15d6-4283-bd6f-a6f814bf3bad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f9b2d433-42af-412f-897b-babf06cc4dc0",
   "metadata": {
    "tags": []
   },
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d31627a-0d66-4056-b367-12a627569d1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adaboost = AdaBoostClassifier(n_estimators = 50, learning_rate = 0.2).fit(X_train, y_train)\n",
    "score = adaboost.score(X_test, y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015029cb-71fc-4799-96cd-7b5d923ef716",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgboost = XGBClassifier(n_estimators = 1000, learning_rate = 0.05).fit(X_train, y_train, early_stopping_rounds = 5, eval_set = [(X_test, y_test)],verbose = False)\n",
    "score_xgb = xgboost.score(X_test,y_test)\n",
    "score_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22a0677-51dc-4d18-9878-10e1ceef90a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68255b0-632c-4652-8b17-8fa0642a81fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5763b9a6-c195-41c4-bb41-8d885613a105",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Poisonous Mushroom   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a319930-a1b9-4950-b2a2-5704d12ad2cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  https://www.kaggle.com/uciml/mushroom-classification\n",
    "df = pd.read_csv(\"../Data/mushrooms.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043d0ccb-bc45-4346-a6e6-a9e775ae6242",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('\\nUnique value count')\n",
    "for col in df.columns:\n",
    "    print(col, len(df[col].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ad1087-0c6d-4d02-a103-4db60df4e274",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# drop a column with only one distinct value\n",
    "df = df.drop(\"veil-type\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb300da2-1efa-4c50-bd8d-3e0ef9879ff6",
   "metadata": {},
   "source": [
    "Convert the dataset to numbers by encoding it.  \n",
    "Scikit-Learn LabelEncoder() converts labels to numbers.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ca1c32-7217-4a53-ac1e-6dd9becc3bd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "for column in df.columns:\n",
    "    df[column] = label_encoder.fit_transform(df[column])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a88afb-8c69-48ce-9851-8b6cfbca1295",
   "metadata": {},
   "source": [
    "Split the dataset into a target matrix Y and a feature matrix X.  \n",
    "Make training and test sets.  \n",
    "Normalise the data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b4d29d-0dcd-4ea4-9075-6e128cd71d5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = df.loc[:, df.columns != 'class']\n",
    "Y = df['class']  \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state = 100)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661c9248-47a9-4bc8-aa5b-154debe3f461",
   "metadata": {},
   "source": [
    "Use AdaBoostClassifier to classify the mushroom  as **poisonous or edible**.\n",
    "\n",
    "The method has the following parameters:\n",
    "\n",
    "1. base_estimator: The boosted ensemble is built from this parameter. If None, the value is DecisionTreeClassifier(max_depth=1).  \n",
    "2. n_estimators: The upper limit in estimators at which boosting is terminated with a default value of 50. If there is a perfect fit, learning is stopped early.  \n",
    "3. learning_rate: The learning rate reduces the contribution of the classifier by this value. It has a default value of 1.  \n",
    "4. algorithm: Default value of `SAMME` (Stagewise Additive Modeling using a Multi-class Exponential loss function).  [Ref](https://hastie.su.domains/Papers/samme.pdf)  \n",
    "5. random_state: Seed used by the random number generator.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62365f09-c536-471f-b966-9d943f76874e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adaboost = AdaBoostClassifier(n_estimators = 50, learning_rate = 0.2).fit(X_train, y_train)\n",
    "score = adaboost.score(X_test, y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6eebbf-1ab4-43a2-8061-1a2e5b199280",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgboost = XGBClassifier(n_estimators = 1000, learning_rate = 0.05).fit(X_train, y_train, early_stopping_rounds = 5, eval_set = [(X_test, y_test)],verbose = False)\n",
    "score_xgb = xgboost.score(X_test,y_test)\n",
    "score_xgb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
